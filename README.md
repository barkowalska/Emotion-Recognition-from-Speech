# Emotion-Recognition-from-Speech

Emotion Recognition from Speech for Smart Affective Services Using a Combination of SVM and DBN


link: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5539696/

Authors: Lianzhang Zhu, Leiming Chen, Dehai Zhao, Jiehan Zhou and Weishan Zhang1

Description of the found data:

link: https://www.kaggle.com/datasets/piyushagni5/berlin-database-of-emotional-speech-emodb?resource=download

The EMODB dataset, developed by the Institute of Communication Science at the Technical University of Berlin, is a publicly accessible collection of German emotional speech recordings. This dataset comprises 535 utterances from ten seasoned speakers, equally divided between male and female. The dataset captures seven distinct emotions: anger, boredom, fear, joy, sadness, disgust, and neutrality. The recordings were initially captured at a 48 kHz sampling rate and subsequently down-sampled to 16 kHz for standardization.
Naming Structure of Files

Each file in the dataset adheres to a specific naming scheme:

    Positions 1-2: Identifier for the speaker
    Positions 3-5: Code representing the text spoken
    Position 6: Letter indicating the emotion (German abbreviation)
    Position 7: Version identifier (if multiple versions exist)

Example: 03a01Fa.wav represents an audio file from Speaker 03, reading text a01, expressing the emotion "Freude" (Joy).
Details About the Speakers

    03: Male, aged 31
    08: Female, aged 34
    09: Female, aged 21
    10: Male, aged 32
    11: Male, aged 26
    12: Male, aged 30
    13: Female, aged 32
    14: Female, aged 35
    15: Male, aged 25
    16: Female, aged 31
